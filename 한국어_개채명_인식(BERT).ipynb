{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "한국어_개채명_인식(BERT).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOggYX6C+kMuEckuN1gyl+X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "66814ac643484441a4a87393e4e48062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_25f49f6b49394fafa37c30c4c7d14a69",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e252106991ee4917bcdddc1fb1fcaff3",
              "IPY_MODEL_f4a2c119c4654c57a5e492ec50faaa31"
            ]
          }
        },
        "25f49f6b49394fafa37c30c4c7d14a69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e252106991ee4917bcdddc1fb1fcaff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fed8ea214d434a65ae00487eb213f487",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 371391,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 371391,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_14addff97f294a19b1a66b1c64198677"
          }
        },
        "f4a2c119c4654c57a5e492ec50faaa31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f9ec2cf0765c4c4f8103cc9fd8766a00",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 371k/371k [00:01&lt;00:00, 337kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_089946fbb56740e18e643a983e659471"
          }
        },
        "fed8ea214d434a65ae00487eb213f487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "14addff97f294a19b1a66b1c64198677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9ec2cf0765c4c4f8103cc9fd8766a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "089946fbb56740e18e643a983e659471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "483021344460461c90c7450e69410376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d0cecbe994294477a2123c838d1333d1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e6eb1933e70e4f26ba17952628fd05ab",
              "IPY_MODEL_5a9be3b7d6ea4d3cbbf25c18b69e8272"
            ]
          }
        },
        "d0cecbe994294477a2123c838d1333d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6eb1933e70e4f26ba17952628fd05ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0fad271f2b224b2787c24993cb82a93d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 77779,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 77779,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cffb6d0563ba419093509707ffe17f1f"
          }
        },
        "5a9be3b7d6ea4d3cbbf25c18b69e8272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f374f2a15a3746cbbbadae15edc2164f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 77.8k/77.8k [00:00&lt;00:00, 494kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7ebd9fc716443d2a83e30d14afcb991"
          }
        },
        "0fad271f2b224b2787c24993cb82a93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cffb6d0563ba419093509707ffe17f1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f374f2a15a3746cbbbadae15edc2164f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7ebd9fc716443d2a83e30d14afcb991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yes-wook/NLP_STRUGGLE/blob/main/%ED%95%9C%EA%B5%AD%EC%96%B4_%EA%B0%9C%EC%B1%84%EB%AA%85_%EC%9D%B8%EC%8B%9D(BERT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9bppIC9RjcE"
      },
      "source": [
        "## 한국어 개체명 인식기 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KHRFgpuWff4"
      },
      "source": [
        "![BERT 모델](https://camo.githubusercontent.com/eea6dc07ad873471be054698e4e3da80aa43d409/68747470733a2f2f692e696d6775722e636f6d2f504464544c6a792e706e67)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBkPuTujXNX-"
      },
      "source": [
        "![링크 텍스트](https://camo.githubusercontent.com/3438ac9a7093d7c42d48da32e0df774eb885079e/68747470733a2f2f692e696d6775722e636f6d2f697434755445332e706e67)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9dOaDzRWfOU"
      },
      "source": [
        "이번 실습은 \r\n",
        "\r\n",
        "- 1) 네이버 개체명 인식 데이터 불러오기 및 전처리 \r\n",
        "- 2) BERT 인풋 만들기 \r\n",
        "- 3) 버트를 활용한 개체명 인식 모델 만들기 \r\n",
        "- 4) 훈련 및 성능 검증 \r\n",
        "- 5) 실제 데이터로 실습하기\r\n",
        "\r\n",
        "로 구성되어 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJuurrczXr3q"
      },
      "source": [
        "### 1)데이터 불러오기 및 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVp3XXxGSVTj",
        "outputId": "c079100b-c641-44bc-e57c-3961e15ea891"
      },
      "source": [
        "!wget https://github.com/naver/nlp-challenge/raw/master/missions/ner/data/train/train_data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-01 07:02:50--  https://github.com/naver/nlp-challenge/raw/master/missions/ner/data/train/train_data\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/naver/nlp-challenge/master/missions/ner/data/train/train_data [following]\n",
            "--2021-02-01 07:02:50--  https://raw.githubusercontent.com/naver/nlp-challenge/master/missions/ner/data/train/train_data\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16945023 (16M) [text/plain]\n",
            "Saving to: ‘train_data’\n",
            "\n",
            "train_data          100%[===================>]  16.16M  48.7MB/s    in 0.3s    \n",
            "\n",
            "2021-02-01 07:02:51 (48.7 MB/s) - ‘train_data’ saved [16945023/16945023]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ue0_I23TMMy",
        "outputId": "08802487-f5e8-4f61-e95c-5eb30e611a40"
      },
      "source": [
        "!pip install transformers\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from transformers import *\r\n",
        "import json\r\n",
        "from tqdm import tqdm\r\n",
        "import os\r\n",
        "import re\r\n",
        "\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 36.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=d75c53c910828755341d9898539124dbc28dd1de1c26be8ea6f5256b6b1a4914\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSQqvSzYTpEz"
      },
      "source": [
        "### Load Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "gEZq1nzWTv0p",
        "outputId": "5b61b15b-ecf9-4409-cd68-912a613eca27"
      },
      "source": [
        "train = pd.read_csv(\"train_data\", names=['src', 'tar'], sep=\"\\t\")\r\n",
        "train = train.reset_index()\r\n",
        "train"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>src</th>\n",
              "      <th>tar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>비토리오</td>\n",
              "      <td>PER_B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>양일</td>\n",
              "      <td>DAT_B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>만에</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>영사관</td>\n",
              "      <td>ORG_B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>감호</td>\n",
              "      <td>CVL_B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769060</th>\n",
              "      <td>2</td>\n",
              "      <td>어째</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769061</th>\n",
              "      <td>3</td>\n",
              "      <td>뭔가</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769062</th>\n",
              "      <td>4</td>\n",
              "      <td>수상쩍은</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769063</th>\n",
              "      <td>5</td>\n",
              "      <td>좌담</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769064</th>\n",
              "      <td>6</td>\n",
              "      <td>．</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>769065 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        index   src    tar\n",
              "0           1  비토리오  PER_B\n",
              "1           2    양일  DAT_B\n",
              "2           3    만에      -\n",
              "3           4   영사관  ORG_B\n",
              "4           5    감호  CVL_B\n",
              "...       ...   ...    ...\n",
              "769060      2    어째      -\n",
              "769061      3    뭔가      -\n",
              "769062      4  수상쩍은      -\n",
              "769063      5    좌담      -\n",
              "769064      6     ．      -\n",
              "\n",
              "[769065 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXvVBlWDY9AS"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxAUXuN7UU0o"
      },
      "source": [
        "## 마침표 확실하게 처리\r\n",
        "train['src'] = train['src'].replace(\". \",\".\", regex=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gP-ZITzYh0l"
      },
      "source": [
        "## 특수 문자 제거\r\n",
        "train['src'] = train['src'].astype(str)\r\n",
        "train['tar'] = train['tar'].astype(str)\r\n",
        "\r\n",
        "train['src'] = train['src'].str.replace(r'[^ㄱ-ㅣ가-힣0-9a-zA-Z.]+', \"\", regex=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KX7Bj6_bUC2"
      },
      "source": [
        "data = [list(x) for x in train[['index', 'src', 'tar']].to_numpy()]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Mst6mrc8AR",
        "outputId": "d2f0122d-ed06-4da1-ff7e-b61d13637882"
      },
      "source": [
        "data[:15]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, '비토리오', 'PER_B'],\n",
              " [2, '양일', 'DAT_B'],\n",
              " [3, '만에', '-'],\n",
              " [4, '영사관', 'ORG_B'],\n",
              " [5, '감호', 'CVL_B'],\n",
              " [6, '용퇴', '-'],\n",
              " [7, '항룡', '-'],\n",
              " [8, '압력설', '-'],\n",
              " [9, '의심만', '-'],\n",
              " [10, '가율', '-'],\n",
              " [1, '이', '-'],\n",
              " [2, '음경동맥의', '-'],\n",
              " [3, '직경이', '-'],\n",
              " [4, '8', 'NUM_B'],\n",
              " [5, '19mm입니다', 'NUM_B']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEMEx5orc_fg"
      },
      "source": [
        "라벨들을 추출하고, 딕셔너리 형태로 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCrEAiu3dJ78"
      },
      "source": [
        "label = train['tar'].unique().tolist()\r\n",
        "label_dict = {word:i for i,word in enumerate(label)}\r\n",
        "label_dict.update({\"[PAD]\":len(label_dict)})\r\n",
        "index_to_ner = {i:j for j,i in label_dict.items()}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iolzI-6IeEiC",
        "outputId": "325eaa85-27ac-4710-915f-f234140c7a35"
      },
      "source": [
        "print(label_dict)\r\n",
        "print(index_to_ner)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'PER_B': 0, 'DAT_B': 1, '-': 2, 'ORG_B': 3, 'CVL_B': 4, 'NUM_B': 5, 'LOC_B': 6, 'EVT_B': 7, 'TRM_B': 8, 'TRM_I': 9, 'EVT_I': 10, 'PER_I': 11, 'CVL_I': 12, 'NUM_I': 13, 'TIM_B': 14, 'TIM_I': 15, 'ORG_I': 16, 'DAT_I': 17, 'ANM_B': 18, 'MAT_B': 19, 'MAT_I': 20, 'AFW_B': 21, 'FLD_B': 22, 'LOC_I': 23, 'AFW_I': 24, 'PLT_B': 25, 'FLD_I': 26, 'ANM_I': 27, 'PLT_I': 28, '[PAD]': 29}\n",
            "{0: 'PER_B', 1: 'DAT_B', 2: '-', 3: 'ORG_B', 4: 'CVL_B', 5: 'NUM_B', 6: 'LOC_B', 7: 'EVT_B', 8: 'TRM_B', 9: 'TRM_I', 10: 'EVT_I', 11: 'PER_I', 12: 'CVL_I', 13: 'NUM_I', 14: 'TIM_B', 15: 'TIM_I', 16: 'ORG_I', 17: 'DAT_I', 18: 'ANM_B', 19: 'MAT_B', 20: 'MAT_I', 21: 'AFW_B', 22: 'FLD_B', 23: 'LOC_I', 24: 'AFW_I', 25: 'PLT_B', 26: 'FLD_I', 27: 'ANM_I', 28: 'PLT_I', 29: '[PAD]'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at0iGROywoo0",
        "outputId": "6a0d1184-1d44-440a-98ce-4511c287401e"
      },
      "source": [
        "data[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, '비토리오', 'PER_B']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqYlmUYPeQSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5f48c3-d131-4f6e-a2a6-61993c74f0c7"
      },
      "source": [
        "## 데이터 문장과 객체로 분리\r\n",
        "### tups에 각 문장별로 list 생성\r\n",
        "tups = []\r\n",
        "temp_tup = [] \r\n",
        "temp_tup.append(data[0][1:])\r\n",
        "\r\n",
        "for i,j,k in data:\r\n",
        "    if i != 1:\r\n",
        "        temp_tup.append([j,label_dict[k]])\r\n",
        "    if i == 1: # 문장 첫 단어,\r\n",
        "        if len(temp_tup) != 0:\r\n",
        "            tups.append(temp_tup)\r\n",
        "            temp_tup = []\r\n",
        "            temp_tup.append([j, label_dict[k]])\r\n",
        "\r\n",
        "# 초기 for문 돌리기 위해 했던 것 삭제\r\n",
        "tups.pop(0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['비토리오', 'PER_B']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyxemiP2x4WT"
      },
      "source": [
        "(단어, 개체) 형태를 (단어,단어,단어) (개체,개체,개체.) 형태로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaLvLDlXyqPX"
      },
      "source": [
        "sentences = [] \r\n",
        "targets = []\r\n",
        "\r\n",
        "for tup in tups:\r\n",
        "    sentence = []\r\n",
        "    target = []\r\n",
        "    sentence.append(\"[CLS]\")\r\n",
        "    target.append(label_dict['-'])\r\n",
        "\r\n",
        "    for i,j in tup:\r\n",
        "        sentence.append(i)\r\n",
        "        target.append(j)\r\n",
        "    \r\n",
        "    sentence.append(\"[SEP]\")\r\n",
        "    target.append(label_dict['-'])\r\n",
        "\r\n",
        "    sentences.append(sentence)\r\n",
        "    targets.append(target)\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6U2_YY4y6wI",
        "outputId": "318e1f8f-c4e2-4358-f161-938cf54d9d4b"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " '비토리오',\n",
              " '양일',\n",
              " '만에',\n",
              " '영사관',\n",
              " '감호',\n",
              " '용퇴',\n",
              " '항룡',\n",
              " '압력설',\n",
              " '의심만',\n",
              " '가율',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwMl89nHztfo",
        "outputId": "cc888c45-eae3-46a1-bfe6-168157679d86"
      },
      "source": [
        "targets[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 0, 1, 2, 3, 4, 2, 2, 2, 2, 2, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLoT1RwV0h_1"
      },
      "source": [
        "### 버트 인풋만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jzWIpVa4USX",
        "outputId": "569e02ce-55b0-4b32-c78a-78537805af92"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 5.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DplCbdU0jrz"
      },
      "source": [
        "- Hubggingface의 monologg 님이 만든 kobert 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pbWp4D407IQ"
      },
      "source": [
        "import logging\r\n",
        "import os\r\n",
        "import unicodedata\r\n",
        "from shutil import copyfile\r\n",
        "\r\n",
        "from transformers import PreTrainedTokenizer\r\n",
        "\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "\r\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\r\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\r\n",
        "\r\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\r\n",
        "    \"vocab_file\": {\r\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\r\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\r\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\r\n",
        "    },\r\n",
        "    \"vocab_txt\": {\r\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\r\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\r\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\r\n",
        "    }\r\n",
        "}\r\n",
        "\r\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\r\n",
        "    \"monologg/kobert\": 512,\r\n",
        "    \"monologg/kobert-lm\": 512,\r\n",
        "    \"monologg/distilkobert\": 512\r\n",
        "}\r\n",
        "\r\n",
        "PRETRAINED_INIT_CONFIGURATION = {\r\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\r\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\r\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\r\n",
        "}\r\n",
        "\r\n",
        "SPIECE_UNDERLINE = u'▁'\r\n",
        "\r\n",
        "\r\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\r\n",
        "    \"\"\"\r\n",
        "        SentencePiece based tokenizer. Peculiarities:\r\n",
        "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\r\n",
        "    \"\"\"\r\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\r\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\r\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\r\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "            self,\r\n",
        "            vocab_file,\r\n",
        "            vocab_txt,\r\n",
        "            do_lower_case=False,\r\n",
        "            remove_space=True,\r\n",
        "            keep_accents=False,\r\n",
        "            unk_token=\"[UNK]\",\r\n",
        "            sep_token=\"[SEP]\",\r\n",
        "            pad_token=\"[PAD]\",\r\n",
        "            cls_token=\"[CLS]\",\r\n",
        "            mask_token=\"[MASK]\",\r\n",
        "            **kwargs):\r\n",
        "        super().__init__(\r\n",
        "            unk_token=unk_token,\r\n",
        "            sep_token=sep_token,\r\n",
        "            pad_token=pad_token,\r\n",
        "            cls_token=cls_token,\r\n",
        "            mask_token=mask_token,\r\n",
        "            **kwargs\r\n",
        "        )\r\n",
        "\r\n",
        "        # Build vocab\r\n",
        "        self.token2idx = dict()\r\n",
        "        self.idx2token = []\r\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\r\n",
        "            for idx, token in enumerate(f):\r\n",
        "                token = token.strip()\r\n",
        "                self.token2idx[token] = idx\r\n",
        "                self.idx2token.append(token)\r\n",
        "\r\n",
        "        try:\r\n",
        "            import sentencepiece as spm\r\n",
        "        except ImportError:\r\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\r\n",
        "                           \"pip install sentencepiece\")\r\n",
        "\r\n",
        "        self.do_lower_case = do_lower_case\r\n",
        "        self.remove_space = remove_space\r\n",
        "        self.keep_accents = keep_accents\r\n",
        "        self.vocab_file = vocab_file\r\n",
        "        self.vocab_txt = vocab_txt\r\n",
        "\r\n",
        "        self.sp_model = spm.SentencePieceProcessor()\r\n",
        "        self.sp_model.Load(vocab_file)\r\n",
        "\r\n",
        "    @property\r\n",
        "    def vocab_size(self):\r\n",
        "        return len(self.idx2token)\r\n",
        "\r\n",
        "    def get_vocab(self):\r\n",
        "        return dict(self.token2idx, **self.added_tokens_encoder)\r\n",
        "\r\n",
        "    def __getstate__(self):\r\n",
        "        state = self.__dict__.copy()\r\n",
        "        state[\"sp_model\"] = None\r\n",
        "        return state\r\n",
        "\r\n",
        "    def __setstate__(self, d):\r\n",
        "        self.__dict__ = d\r\n",
        "        try:\r\n",
        "            import sentencepiece as spm\r\n",
        "        except ImportError:\r\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\r\n",
        "                           \"pip install sentencepiece\")\r\n",
        "        self.sp_model = spm.SentencePieceProcessor()\r\n",
        "        self.sp_model.Load(self.vocab_file)\r\n",
        "\r\n",
        "    def preprocess_text(self, inputs):\r\n",
        "        if self.remove_space:\r\n",
        "            outputs = \" \".join(inputs.strip().split())\r\n",
        "        else:\r\n",
        "            outputs = inputs\r\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\r\n",
        "\r\n",
        "        if not self.keep_accents:\r\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\r\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\r\n",
        "        if self.do_lower_case:\r\n",
        "            outputs = outputs.lower()\r\n",
        "\r\n",
        "        return outputs\r\n",
        "\r\n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\r\n",
        "        \"\"\" Tokenize a string. \"\"\"\r\n",
        "        text = self.preprocess_text(text)\r\n",
        "\r\n",
        "        if not sample:\r\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\r\n",
        "        else:\r\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\r\n",
        "        new_pieces = []\r\n",
        "        for piece in pieces:\r\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\r\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\r\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\r\n",
        "                    if len(cur_pieces[0]) == 1:\r\n",
        "                        cur_pieces = cur_pieces[1:]\r\n",
        "                    else:\r\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\r\n",
        "                cur_pieces.append(piece[-1])\r\n",
        "                new_pieces.extend(cur_pieces)\r\n",
        "            else:\r\n",
        "                new_pieces.append(piece)\r\n",
        "\r\n",
        "        return new_pieces\r\n",
        "\r\n",
        "    def _convert_token_to_id(self, token):\r\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\r\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\r\n",
        "\r\n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\r\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\r\n",
        "        return self.idx2token[index]\r\n",
        "\r\n",
        "    def convert_tokens_to_string(self, tokens):\r\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\r\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\r\n",
        "        return out_string\r\n",
        "\r\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\r\n",
        "        \"\"\"\r\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\r\n",
        "        by concatenating and adding special tokens.\r\n",
        "        A KoBERT sequence has the following format:\r\n",
        "            single sequence: [CLS] X [SEP]\r\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\r\n",
        "        \"\"\"\r\n",
        "        if token_ids_1 is None:\r\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\r\n",
        "        cls = [self.cls_token_id]\r\n",
        "        sep = [self.sep_token_id]\r\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\r\n",
        "\r\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\r\n",
        "        \"\"\"\r\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\r\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\r\n",
        "        Args:\r\n",
        "            token_ids_0: list of ids (must not contain special tokens)\r\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\r\n",
        "                for sequence pairs\r\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\r\n",
        "                special tokens for the model\r\n",
        "        Returns:\r\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        if already_has_special_tokens:\r\n",
        "            if token_ids_1 is not None:\r\n",
        "                raise ValueError(\r\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\r\n",
        "                    \"ids is already formated with special tokens for the model.\"\r\n",
        "                )\r\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\r\n",
        "\r\n",
        "        if token_ids_1 is not None:\r\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\r\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\r\n",
        "\r\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\r\n",
        "        \"\"\"\r\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\r\n",
        "        A KoBERT sequence pair mask has the following format:\r\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\r\n",
        "        | first sequence    | second sequence\r\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\r\n",
        "        \"\"\"\r\n",
        "        sep = [self.sep_token_id]\r\n",
        "        cls = [self.cls_token_id]\r\n",
        "        if token_ids_1 is None:\r\n",
        "            return len(cls + token_ids_0 + sep) * [0]\r\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\r\n",
        "\r\n",
        "    def save_vocabulary(self, save_directory):\r\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\r\n",
        "            to a directory.\r\n",
        "        \"\"\"\r\n",
        "        if not os.path.isdir(save_directory):\r\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\r\n",
        "            return\r\n",
        "\r\n",
        "        # 1. Save sentencepiece model\r\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\r\n",
        "\r\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\r\n",
        "            copyfile(self.vocab_file, out_vocab_model)\r\n",
        "\r\n",
        "        # 2. Save vocab.txt\r\n",
        "        index = 0\r\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\r\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\r\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\r\n",
        "                if index != token_index:\r\n",
        "                    logger.warning(\r\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\r\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\r\n",
        "                    )\r\n",
        "                    index = token_index\r\n",
        "                writer.write(token + \"\\n\")\r\n",
        "                index += 1\r\n",
        "\r\n",
        "        return out_vocab_model, out_vocab_txt"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114,
          "referenced_widgets": [
            "66814ac643484441a4a87393e4e48062",
            "25f49f6b49394fafa37c30c4c7d14a69",
            "e252106991ee4917bcdddc1fb1fcaff3",
            "f4a2c119c4654c57a5e492ec50faaa31",
            "fed8ea214d434a65ae00487eb213f487",
            "14addff97f294a19b1a66b1c64198677",
            "f9ec2cf0765c4c4f8103cc9fd8766a00",
            "089946fbb56740e18e643a983e659471",
            "483021344460461c90c7450e69410376",
            "d0cecbe994294477a2123c838d1333d1",
            "e6eb1933e70e4f26ba17952628fd05ab",
            "5a9be3b7d6ea4d3cbbf25c18b69e8272",
            "0fad271f2b224b2787c24993cb82a93d",
            "cffb6d0563ba419093509707ffe17f1f",
            "f374f2a15a3746cbbbadae15edc2164f",
            "c7ebd9fc716443d2a83e30d14afcb991"
          ]
        },
        "id": "FanIFW843U-l",
        "outputId": "44d4bfe5-032d-41e7-fc3f-11e582d217f1"
      },
      "source": [
        "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66814ac643484441a4a87393e4e48062",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=371391.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "483021344460461c90c7450e69410376",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=77779.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKSj0jBN4YfV",
        "outputId": "2d19d081-a34c-4fe5-847c-ea386cedac60"
      },
      "source": [
        "tokenizer.tokenize(\"롯데정보통신 잘하자.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁롯데', '정보', '통신', '▁잘', '하자', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZXKuQ5P8XZZ"
      },
      "source": [
        "문장을 토크나이징한 후, target을 문장에 맞춰야함 \r\n",
        "- ex) (_롯데, 개체1), (정보, 개체1), (통신,개체1), (_잘, 개체2), (하자, 개체2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7L7xFWY9dWl"
      },
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels):\r\n",
        "    tokenized_sentence = []\r\n",
        "    labels = []\r\n",
        "\r\n",
        "    for word, label in zip(sentence, text_labels):\r\n",
        "        tokenized_word = tokenizer.tokenize(word) # 기존의 text를 wordpiece 토크나이징\r\n",
        "        n_subwords = len(tokenized_word)\r\n",
        "\r\n",
        "        tokenized_sentence.extend(tokenized_word)\r\n",
        "        labels.extend([label]*n_subwords)\r\n",
        "\r\n",
        "    return tokenized_sentence, labels"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIoZW8Q2-gOR"
      },
      "source": [
        "tokenized_texts_and_labels = [tokenize_and_preserve_labels(sent, labs) for sent, labs in zip(sentences, targets)]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0goTSMil-jDs",
        "outputId": "58834d8f-82f3-4fa4-b344-8002dd14308e"
      },
      "source": [
        "print(tokenized_texts_and_labels[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['[CLS]', '▁비', '토', '리', '오', '▁양', '일', '▁만에', '▁영', '사', '관', '▁감', '호', '▁용', '퇴', '▁항', '룡', '▁압력', '설', '▁의심', '만', '▁', '가', '율', '[SEP]'], [2, 0, 0, 0, 0, 1, 1, 2, 3, 3, 3, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3zurzPWDKq0"
      },
      "source": [
        "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\r\n",
        "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtqNZzQ1Du2S",
        "outputId": "a19bee46-4c46-4266-99f4-d8db090ed01b"
      },
      "source": [
        "print(tokenized_texts[0])\r\n",
        "print(len(tokenized_texts))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', '▁비', '토', '리', '오', '▁양', '일', '▁만에', '▁영', '사', '관', '▁감', '호', '▁용', '퇴', '▁항', '룡', '▁압력', '설', '▁의심', '만', '▁', '가', '율', '[SEP]']\n",
            "65520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "RUciUwXvD7l7",
        "outputId": "0ea3dd58-626b-449c-c951-40b628a93641"
      },
      "source": [
        "# 문장의 길이를 알아보자\r\n",
        "sent_len = [len(sent) for sent in tokenized_texts]\r\n",
        "\r\n",
        "import seaborn as sns\r\n",
        "sns.boxplot(sent_len)\r\n",
        "\r\n",
        "pd.Series(sent_len).describe()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    65520.000000\n",
              "mean        52.313507\n",
              "std        302.518523\n",
              "min          2.000000\n",
              "25%         19.000000\n",
              "50%         27.000000\n",
              "75%         39.000000\n",
              "max      15918.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PydSymrjItQw",
        "outputId": "7deb36dd-cb8a-46f2-8f8c-fbc844f0d319"
      },
      "source": [
        "print(np.quantile(np.array([len(x) for x in tokenized_texts]), 0.975))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVdknluNJK6U"
      },
      "source": [
        "88개의 문장으로 max_len 설정하면, 상위 2.5퍼센트를 제외하고 커버가능\r\n",
        "\r\n",
        "max_len은 88로 잡고, input_ids와 attention_masks로 구성된 input 만들기\r\n",
        "\r\n",
        "senten_id는 한문장이기에 제외"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bss0Q1YaQwCr"
      },
      "source": [
        "max_len = 88\r\n",
        "bs = 32"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFZdI2veJRkn"
      },
      "source": [
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\r\n",
        "                          maxlen = max_len, dtype = 'int', value = tokenizer.convert_tokens_to_ids(\"[PAD]\"),\r\n",
        "                          truncating = \"post\", padding = 'post')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "D45pScnzQu8B",
        "outputId": "dc71b1cf-4de9-440e-9217-198eb1619cc2"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(4)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[MASK]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlHMIzVtTjo_",
        "outputId": "f30bff32-35c0-4352-c4f0-74134c2c239c"
      },
      "source": [
        "label_dict['[PAD]']"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyDWzYXXUmV6"
      },
      "source": [
        "정답 집합 tags 만들기 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZYmF-qyVIb2"
      },
      "source": [
        "tags = pad_sequences([lab for lab in labels], maxlen = max_len, \r\n",
        "                     value = label_dict['[PAD]'], padding = 'post', dtype = 'int',\r\n",
        "                     truncating = 'post')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAJ9W-MXVc1G",
        "outputId": "a0acc984-e74e-462d-8458-3620f75ab787"
      },
      "source": [
        "tags[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2,  0,  0,  0,  0,  1,  1,  2,  3,  3,  3,  4,  4,  2,  2,  2,  2,\n",
              "        2,  2,  2,  2,  2,  2,  2,  2, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
              "       29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
              "       29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
              "       29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
              "       29, 29, 29])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vLBIQ9iXavL",
        "outputId": "4f0fc5cc-ffce-4ca3-a6bd-46ddef44e8fc"
      },
      "source": [
        "int(1 != 0)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UccNZHc6WeUq"
      },
      "source": [
        "## PAD는 0 아니면 1로 바꿈\r\n",
        "attention_masks = np.array([[int(id != tokenizer.convert_tokens_to_ids(\"[PAD]\")) for id in ii] for ii in input_ids] )"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Frwl1f8vYlmp",
        "outputId": "b41a63b5-4197-4d20-9d6c-c59c3d9ed12e"
      },
      "source": [
        "attention_masks"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX5jqsMvYnCU"
      },
      "source": [
        "Train/Test Split 하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW6lBjhLYqfK"
      },
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, random_state = 2018, test_size = 0.1)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtkCtVZvZB6s"
      },
      "source": [
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.1)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH0ChXvcZLCZ"
      },
      "source": [
        "### 개체명 인식 모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx8Ul2KGZOPM"
      },
      "source": [
        "# TPU 작동을 위한 실행"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5QDIAp1ZRWN",
        "outputId": "81713abe-d2da-414a-c126-8db7ff7543d2"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n",
        "tf.config.experimental_connect_to_cluster(resolver)\r\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.10.134.90:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.10.134.90:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.10.134.90:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.10.134.90:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.tpu.topology.Topology at 0x7f57cbe30ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3RxvuXiZyRl"
      },
      "source": [
        "SEQ_LEN = max_len\r\n",
        "\r\n",
        "def create_model():\r\n",
        "    model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt = True, num_labels = len(label_dict), output_attentions = False, output_hidden_states = False)\r\n",
        "    token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype = tf.int32, name = 'input_word_ids') #토큰 인풋\r\n",
        "    mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype = tf.int32, name = 'input_masks') #토큰 인풋\r\n",
        "\r\n",
        "    bert_outputs = model([token_inputs, mask_inputs])\r\n",
        "    bert_outputs = bert_outputs[0] # shape : (Batch_size, max_len, 30(개체의 총 갯수))\r\n",
        "    nr = tf.keras.layers.Dense(30, activation = 'softmax')(bert_outputs) # shape : (Batch_size, max_len, 30)\r\n",
        "\r\n",
        "    nr_model = tf.keras.Model([token_inputs, mask_inputs], nr)\r\n",
        "\r\n",
        "    nr_model.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.00002),\r\n",
        "                     loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \r\n",
        "                     metrics = ['sparse_categorical_accuracy'])\r\n",
        "\r\n",
        "    nr_model.summary()\r\n",
        "\r\n",
        "    return nr_model"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm1G44LqZTKR"
      },
      "source": [
        "## 훈련 및 성능 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbTKGXjVmTcH",
        "outputId": "7ecf132a-55e0-4fda-b2c0-6d92a3c8fe79"
      },
      "source": [
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n",
        "\r\n",
        "#TPU 활용을 위해 context로 묶기\r\n",
        "with strategy.scope():\r\n",
        "    nr_model = create_model()\r\n",
        "    nr_model.fit([tr_inputs, tr_masks], tr_tags, validation_data = ([val_inputs, val_masks], val_tags), epochs = 3, shuffle = False, batch_size = bs)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "All PyTorch model weights were used when initializing TFBertModel.\n",
            "\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 88)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 88)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_4 (TFBertModel)   TFBaseModelOutputWit 92186880    input_word_ids[0][0]             \n",
            "                                                                 input_masks[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 88, 30)       23070       tf_bert_model_4[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 92,209,950\n",
            "Trainable params: 92,209,950\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1843/1843 [==============================] - ETA: 0s - loss: 0.4152 - sparse_categorical_accuracy: 0.8965"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1843/1843 [==============================] - 232s 90ms/step - loss: 0.4151 - sparse_categorical_accuracy: 0.8965 - val_loss: 0.1912 - val_sparse_categorical_accuracy: 0.9486\n",
            "Epoch 2/3\n",
            "1843/1843 [==============================] - 127s 69ms/step - loss: 0.1820 - sparse_categorical_accuracy: 0.9516 - val_loss: 0.1754 - val_sparse_categorical_accuracy: 0.9531\n",
            "Epoch 3/3\n",
            "1843/1843 [==============================] - 125s 68ms/step - loss: 0.1510 - sparse_categorical_accuracy: 0.9599 - val_loss: 0.1718 - val_sparse_categorical_accuracy: 0.9549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwqS0Zl_nTRs"
      },
      "source": [
        "### 성능 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAe0xx5VsSJq",
        "outputId": "f3bf1dd8-855e-4209-d990-c0d206d15ffe"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\r\n",
        "y_predicted = nr_model.predict([val_inputs, val_masks])\r\n",
        "\r\n",
        "f_label = [i for i, j in label_dict.items()]\r\n",
        "val_tags_l = [index_to_ner[x] for x in np.ravel(val_tags).astype(int).tolist()]\r\n",
        "y_predicted_l = [index_to_ner[x] for x in np.ravel(np.argmax(y_predicted, axis=2)).astype(int).tolist()]\r\n",
        "f_label.remove(\"[PAD]\")"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0q-shsXsnFw"
      },
      "source": [
        "- 각 개체별 f1 score를 측정하도록 하겠습니다.\r\n",
        "- 참고로 micro avg는 전체 정답을 기준으로 f1 score을 측정한 것이며,\r\n",
        "- macro avg는 각 개체별 f1 score를 가중평균 한 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LmIcYWRsbfV",
        "outputId": "cb7bbe1f-f88e-4a7a-90dd-c622dbb2512e"
      },
      "source": [
        "print(classification_report(val_tags_l, y_predicted_l, labels=f_label))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       PER_B       0.88      0.85      0.87     11330\n",
            "       DAT_B       0.90      0.84      0.87      4239\n",
            "           -       0.95      0.91      0.93    127187\n",
            "       ORG_B       0.88      0.81      0.84     11615\n",
            "       CVL_B       0.79      0.86      0.82     14438\n",
            "       NUM_B       0.95      0.90      0.93     10409\n",
            "       LOC_B       0.85      0.80      0.83      5621\n",
            "       EVT_B       0.79      0.80      0.80      3122\n",
            "       TRM_B       0.88      0.79      0.83      6993\n",
            "       TRM_I       0.45      0.43      0.44       681\n",
            "       EVT_I       0.80      0.76      0.78      1545\n",
            "       PER_I       0.70      0.73      0.72      1573\n",
            "       CVL_I       0.48      0.44      0.46       784\n",
            "       NUM_I       0.64      0.79      0.71      1461\n",
            "       TIM_B       0.83      0.89      0.86       547\n",
            "       TIM_I       0.86      0.86      0.86       224\n",
            "       ORG_I       0.53      0.88      0.66      1326\n",
            "       DAT_I       0.85      0.92      0.89      1123\n",
            "       ANM_B       0.69      0.75      0.72      1416\n",
            "       MAT_B       0.43      0.28      0.34        54\n",
            "       MAT_I       0.00      0.00      0.00         0\n",
            "       AFW_B       0.61      0.61      0.61      1122\n",
            "       FLD_B       0.52      0.55      0.53       413\n",
            "       LOC_I       0.00      0.00      0.00        37\n",
            "       AFW_I       0.49      0.65      0.56       446\n",
            "       PLT_B       0.26      0.09      0.14        65\n",
            "       FLD_I       0.00      0.00      0.00         2\n",
            "       ANM_I       0.00      0.00      0.00         7\n",
            "       PLT_I       0.00      0.00      0.00         0\n",
            "\n",
            "   micro avg       0.90      0.87      0.89    207780\n",
            "   macro avg       0.59      0.59      0.59    207780\n",
            "weighted avg       0.91      0.87      0.89    207780\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxqCYk4Cs1Mr"
      },
      "source": [
        "### 실제 데이터 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa-iiP3dtkvL",
        "outputId": "532bb201-6ae6-405e-89cb-699050335d5d"
      },
      "source": [
        "np.array([tokenizer.encode(\"아 이제 퇴근하고 싶다\", max_length=max_len, pad_to_max_length=True)])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   2, 3093, 3742, 4760, 5546, 7788, 3072,    3,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csotceyBs3H4"
      },
      "source": [
        "def ner_inference(test_sentence):\r\n",
        "    tokenized_sentence = np.array([tokenizer.encode(test_sentence, max_length=max_len, pad_to_max_length=True)])\r\n",
        "    tokenized_mask = np.array([[ int(x!=1) for x in tokenized_sentence[0].tolist()]])\r\n",
        "    \r\n",
        "    ans = nr_model.predict([tokenized_sentence, tokenized_mask])\r\n",
        "    ans = np.argmax(ans, axis = 2)\r\n",
        "\r\n",
        "    tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence[0])\r\n",
        "    new_tokens, new_labels = [], []\r\n",
        "    for token, label_idx in zip(tokens, ans[0]):\r\n",
        "  \r\n",
        "        if (token.startswith(\"▁\")):\r\n",
        "            new_labels.append(index_to_ner[label_idx])\r\n",
        "            new_tokens.append(token[1:])\r\n",
        "        elif (token=='[CLS]'):\r\n",
        "            pass\r\n",
        "        elif (token=='[SEP]'):\r\n",
        "            pass\r\n",
        "        elif (token=='[PAD]'):\r\n",
        "            pass\r\n",
        "        elif (token != '[CLS]' or token != '[SEP]'):\r\n",
        "            new_tokens[-1] = new_tokens[-1] + token\r\n",
        "\r\n",
        "    for token, label in zip(new_tokens, new_labels):\r\n",
        "        print(\"{}\\t{}\".format(label, token))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_grqxC9x1bZ2"
      },
      "source": [
        "test_sentence = \"김태규 부산지법 부장판사는 31일 본인 페이스북에 올린 글에서 이같이 주장하며 “관료로 임명되고 정치와 가장 먼 영역에 있는 법원에까지 탄핵의 칼날을 들이대는 것은 이제 이것을 아주 편하게 얼마든지 쓰겠다는 선언에 지나지 않는다”고 비판했다.\""
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqWC-ayo2Jv5",
        "outputId": "3d9d38d0-7119-49b7-ed14-6fcaa336be6e"
      },
      "source": [
        "ans1[0][0]"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8.3422856e-05, 2.0603218e-05, 9.9922061e-01, 3.1188894e-05,\n",
              "       7.4360018e-05, 1.1333326e-04, 1.9949286e-05, 3.0825908e-05,\n",
              "       4.6752575e-05, 5.7035131e-06, 8.9578607e-06, 7.1025738e-06,\n",
              "       1.0434255e-05, 4.5592169e-05, 1.3401437e-05, 9.9159015e-06,\n",
              "       1.0906647e-05, 1.0751217e-05, 2.1262818e-05, 7.9658575e-06,\n",
              "       2.5078853e-06, 2.2102498e-05, 1.6377247e-05, 6.1855490e-06,\n",
              "       2.0578746e-05, 1.1428886e-05, 5.1029087e-06, 2.3785615e-06,\n",
              "       1.7499057e-06, 1.1859759e-04], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8WbITgn1rhb",
        "outputId": "22961c63-33d7-4ae9-cd23-a2ed7bb89b7c"
      },
      "source": [
        "tokenized_sentence = np.array([tokenizer.encode(test_sentence, max_length=max_len, pad_to_max_length=True)])\r\n",
        "tokenized_mask = np.array([[int(x!=1) for x in tokenized_sentence[0].tolist()]])\r\n",
        "\r\n",
        "ans1 = nr_model.predict([tokenized_sentence, tokenized_mask])\r\n",
        "ans = np.argmax(ans1, axis = 2) ## 30개 중에 가장 높음 확률을 찾음\r\n",
        "\r\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence[0])\r\n",
        "new_tokens, new_labels = [], []\r\n",
        "for token, label_idx in zip(tokens, ans[0]):\r\n",
        "\r\n",
        "    if (token.startswith(\"▁\")):\r\n",
        "        new_labels.append(index_to_ner[label_idx])\r\n",
        "        new_tokens.append(token[1:])\r\n",
        "    elif (token=='[CLS]'):\r\n",
        "        pass\r\n",
        "    elif (token=='[SEP]'):\r\n",
        "        pass\r\n",
        "    elif (token=='[PAD]'):\r\n",
        "        pass\r\n",
        "    elif (token != '[CLS]' or token != '[SEP]'):\r\n",
        "        new_tokens[-1] = new_tokens[-1] + token\r\n",
        "\r\n",
        "for token, label in zip(new_tokens, new_labels):\r\n",
        "    print(\"{}\\t{}\".format(label, token))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "PER_B\t김태규\n",
            "ORG_B\t부산지법\n",
            "CVL_B\t부장판사는\n",
            "DAT_B\t31일\n",
            "-\t본인\n",
            "-\t페이스북에\n",
            "-\t올린\n",
            "-\t글에서\n",
            "-\t이같이\n",
            "-\t주장하며\n",
            "CVL_B\t“관료로\n",
            "-\t임명되고\n",
            "-\t정치와\n",
            "-\t가장\n",
            "-\t먼\n",
            "-\t영역에\n",
            "-\t있는\n",
            "-\t법원에까지\n",
            "-\t탄핵의\n",
            "-\t칼날을\n",
            "-\t들이대는\n",
            "-\t것은\n",
            "-\t이제\n",
            "-\t이것을\n",
            "-\t아주\n",
            "-\t편하게\n",
            "-\t얼마든지\n",
            "-\t쓰겠다는\n",
            "-\t선언에\n",
            "-\t지나지\n",
            "-\t않는다”고\n",
            "-\t비판했다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR3qz1iqx1Xg",
        "outputId": "aed6bfba-7152-4213-cbf2-50049e04f35e"
      },
      "source": [
        "ner_inference(\"송중기 시대극은 믿고본다. 첫회 신선하고 좋았다.\")"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "PER_B\t송중기\n",
            "-\t시대극은\n",
            "-\t믿고본다.\n",
            "NUM_B\t첫회\n",
            "-\t신선하고\n",
            "-\t좋았다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bquz7mSix4FV",
        "outputId": "67e6e715-3d61-4131-beb5-bee401c0fef4"
      },
      "source": [
        "ner_inference(\"\"\"안녕하세요 NLP Engineer로 활동하고 있는 박장원입니다:)\r\n",
        "\r\n",
        "원래 제 전공은 경영학과입니다. 우연한 계기로 자연어 처리를 접하게 되어 현재는 NLP Engineer로 살아가고 있습니다.\r\n",
        "\r\n",
        "한국어 NLP에 더 많은 기여를 하기 위해 오늘도 달립니다!!\"\"\")"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-\t안녕하세요\n",
            "CVL_B\tNLP\n",
            "CVL_I\tEngineer로\n",
            "-\t활동하고\n",
            "-\t있는\n",
            "PER_B\t박장원입니다:)\n",
            "-\t원래\n",
            "-\t제\n",
            "-\t전공은\n",
            "ORG_B\t경영학과입니다.\n",
            "-\t우연한\n",
            "-\t계기로\n",
            "-\t자연어\n",
            "-\t처리를\n",
            "-\t접하게\n",
            "-\t되어\n",
            "-\t현재는\n",
            "CVL_B\tNLP\n",
            "CVL_B\tEngineer로\n",
            "-\t살아가고\n",
            "-\t있습니다.\n",
            "CVL_B\t한국어\n",
            "CVL_B\tNLP에\n",
            "-\t더\n",
            "-\t많은\n",
            "-\t기여를\n",
            "-\t하기\n",
            "-\t위해\n",
            "DAT_B\t오늘도\n",
            "-\t달립니다!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VMb1lNPybHU",
        "outputId": "76ce39c6-25ec-4ec8-9bf2-954d6d6f51b7"
      },
      "source": [
        "ner_inference(\"알바쓰고많이만들면되지 돈욕심없으면골목식당왜나온겨 기댕기게나하고 산에가서팔어라\")"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-\t알바쓰고많이만들면되지\n",
            "-\t돈욕심없으면골목식당왜나온겨\n",
            "-\t기[UNK]기게나하고\n",
            "-\t산에가서팔어라\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYfZ8sZWyczO",
        "outputId": "91e248af-31f0-4439-aa91-8444718c9df6"
      },
      "source": [
        "ner_inference(\"인공지능의 역사는 20세기 초반에서 더 거슬러 올라가보면 이미 17~18세기부터 태동하고 있었지만 이때는 인공지능 그 자체보다는 뇌와 마음의 관계에 관한 철학적인 논쟁 수준에 머무르고 있었다. 그럴 수 밖에 없는 것이 당시에는 인간의 뇌 말고는 정보처리기계가 존재하지 않았기 때문이다. \")"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRM_B\t인공지능의\n",
            "-\t역사는\n",
            "DAT_B\t20세기\n",
            "-\t초반에서\n",
            "-\t더\n",
            "-\t거슬러\n",
            "-\t올라가보면\n",
            "-\t이미\n",
            "NUM_B\t17~18세기부터\n",
            "-\t태동하고\n",
            "-\t있었지만\n",
            "-\t이때는\n",
            "-\t인공지능\n",
            "-\t그\n",
            "-\t자체보다는\n",
            "-\t뇌와\n",
            "-\t마음의\n",
            "-\t관계에\n",
            "-\t관한\n",
            "-\t철학적인\n",
            "-\t논쟁\n",
            "-\t수준에\n",
            "-\t머무르고\n",
            "-\t있었다.\n",
            "-\t그럴\n",
            "-\t수\n",
            "-\t밖에\n",
            "-\t없는\n",
            "-\t것이\n",
            "-\t당시에는\n",
            "-\t인간의\n",
            "-\t뇌\n",
            "-\t말고는\n",
            "TRM_B\t정보처리기계가\n",
            "-\t존재하지\n",
            "-\t않았기\n",
            "-\t때문이다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ufSJbBymm1"
      },
      "source": [
        ""
      ],
      "execution_count": 102,
      "outputs": []
    }
  ]
}